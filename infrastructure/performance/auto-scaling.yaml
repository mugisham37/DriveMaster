# Auto-scaling Configuration for Adaptive Learning Platform
# Implements Horizontal Pod Autoscaler (HPA) and Vertical Pod Autoscaler (VPA)

apiVersion: v1
kind: Namespace
metadata:
  name: adaptive-learning

---
# Horizontal Pod Autoscaler for Scheduler Service
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: scheduler-service-hpa
  namespace: adaptive-learning
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: scheduler-service
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: requests_per_second
      target:
        type: AverageValue
        averageValue: "100"
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
      - type: Pods
        value: 4
        periodSeconds: 15
      selectPolicy: Max
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
      selectPolicy: Min

---
# HPA for User Service
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: user-service-hpa
  namespace: adaptive-learning
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: user-service
  minReplicas: 2
  maxReplicas: 15
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 75
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 15
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60

---
# HPA for Content Service
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: content-service-hpa
  namespace: adaptive-learning
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: content-service
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 75
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80

---
# HPA for ML Service
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ml-service-hpa
  namespace: adaptive-learning
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ml-service
  minReplicas: 2
  maxReplicas: 8
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 80
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 85
  - type: Pods
    pods:
      metric:
        name: inference_queue_length
      target:
        type: AverageValue
        averageValue: "10"

---
# Vertical Pod Autoscaler for Scheduler Service
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: scheduler-service-vpa
  namespace: adaptive-learning
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: scheduler-service
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: scheduler-service
      minAllowed:
        cpu: 100m
        memory: 128Mi
      maxAllowed:
        cpu: 2
        memory: 4Gi
      controlledResources: ["cpu", "memory"]

---
# VPA for User Service
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: user-service-vpa
  namespace: adaptive-learning
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: user-service
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: user-service
      minAllowed:
        cpu: 100m
        memory: 128Mi
      maxAllowed:
        cpu: 1
        memory: 2Gi

---
# Custom Resource for Advanced Auto-scaling
apiVersion: v1
kind: ConfigMap
metadata:
  name: autoscaling-config
  namespace: adaptive-learning
data:
  config.yaml: |
    # Advanced auto-scaling configuration
    scaling_policies:
      scheduler_service:
        min_replicas: 3
        max_replicas: 20
        target_cpu: 70
        target_memory: 80
        scale_up_cooldown: 60s
        scale_down_cooldown: 300s
        
        # Custom metrics thresholds
        custom_metrics:
          - name: "requests_per_second"
            target: 100
            window: "5m"
          - name: "response_time_p95"
            target: 300  # milliseconds
            window: "2m"
          - name: "queue_depth"
            target: 50
            window: "1m"
            
        # Predictive scaling
        predictive_scaling:
          enabled: true
          look_ahead: "15m"
          historical_data: "7d"
          
      user_service:
        min_replicas: 2
        max_replicas: 15
        target_cpu: 70
        target_memory: 75
        
      content_service:
        min_replicas: 2
        max_replicas: 10
        target_cpu: 75
        target_memory: 80
        
        # Content service specific metrics
        custom_metrics:
          - name: "cache_hit_ratio"
            target: 0.85
            action: "scale_down_if_above"
          - name: "search_latency_p95"
            target: 100  # milliseconds
            
      ml_service:
        min_replicas: 2
        max_replicas: 8
        target_cpu: 80
        target_memory: 85
        
        # ML service specific configuration
        gpu_scaling:
          enabled: true
          min_gpu_replicas: 1
          max_gpu_replicas: 4
          
        custom_metrics:
          - name: "inference_queue_length"
            target: 10
          - name: "model_accuracy"
            target: 0.85
            action: "alert_if_below"

---
# Pod Disruption Budget for High Availability
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: scheduler-service-pdb
  namespace: adaptive-learning
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: scheduler-service

---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: user-service-pdb
  namespace: adaptive-learning
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: user-service

---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: content-service-pdb
  namespace: adaptive-learning
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: content-service

---
# Cluster Autoscaler Configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-autoscaler-config
  namespace: kube-system
data:
  nodes.max: "50"
  nodes.min: "3"
  scale-down-delay-after-add: "10m"
  scale-down-unneeded-time: "10m"
  scale-down-utilization-threshold: "0.5"
  skip-nodes-with-local-storage: "false"
  skip-nodes-with-system-pods: "false"

---
# Network Policy for Auto-scaled Pods
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: autoscaling-network-policy
  namespace: adaptive-learning
spec:
  podSelector:
    matchLabels:
      tier: backend
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          tier: frontend
    - podSelector:
        matchLabels:
          tier: backend
    ports:
    - protocol: TCP
      port: 8080
    - protocol: TCP
      port: 50051  # gRPC
  egress:
  - to:
    - podSelector:
        matchLabels:
          tier: database
    ports:
    - protocol: TCP
      port: 5432
  - to:
    - podSelector:
        matchLabels:
          tier: cache
    ports:
    - protocol: TCP
      port: 6379

---
# Resource Quotas for Auto-scaling
apiVersion: v1
kind: ResourceQuota
metadata:
  name: autoscaling-quota
  namespace: adaptive-learning
spec:
  hard:
    requests.cpu: "20"
    requests.memory: 40Gi
    limits.cpu: "40"
    limits.memory: 80Gi
    pods: "100"
    persistentvolumeclaims: "20"
    services: "20"
    secrets: "20"
    configmaps: "20"

---
# Limit Range for Pod Resources
apiVersion: v1
kind: LimitRange
metadata:
  name: autoscaling-limits
  namespace: adaptive-learning
spec:
  limits:
  - default:
      cpu: 500m
      memory: 1Gi
    defaultRequest:
      cpu: 100m
      memory: 128Mi
    type: Container
  - max:
      cpu: 4
      memory: 8Gi
    min:
      cpu: 50m
      memory: 64Mi
    type: Container